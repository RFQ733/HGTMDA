{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/RFQ733/HGTMDA.git\n",
    "!pwd\n",
    "%cd ./HGTMDA/\n",
    "%pip install torch_geometric\n",
    "%pip install  dgl -f https://data.dgl.ai/wheels/cu118/repo.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from model import MDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import optim, nn\n",
    "import torch as t\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, average_precision_score, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "import argparse\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"  \n",
    "m_odel_dir = \"./save_model/5_fold\"  \n",
    "\n",
    "if not os.path.exists(m_odel_dir):\n",
    "    os.makedirs(m_odel_dir)\n",
    "device = t.device('cuda:0' if t.cuda.is_available() else \"cpu\")\n",
    "t.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--epochs N] [--wd WD] [--lr LR]\n",
      "                             [--hid_feats HID_FEATS] [--out_feats OUT_FEATS]\n",
      "                             [--method METHOD] [--num_layers NUM_LAYERS]\n",
      "                             [--input_dropout INPUT_DROPOUT]\n",
      "                             [--layer_dropout LAYER_DROPOUT]\n",
      "                             [--random_seed RANDOM_SEED] [--k K]\n",
      "                             [--early_stopping EARLY_STOPPING]\n",
      "                             [--dropout DROPOUT] [--mlp MLP]\n",
      "                             [--neighbor NEIGHBOR] [--save_score SAVE_SCORE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\rfq\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-2703681FlnloiIyPh.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(m_odel_dir):\n",
    "    os.makedirs(m_odel_dir)\n",
    "device = t.device('cuda:0' if t.cuda.is_available() else \"cpu\")\n",
    "t.backends.cudnn.enabled = True\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', type=int, default=200, metavar='N', help='number of epochs to train')\n",
    "parser.add_argument('--wd', type=float, default=1e-3, help='weight_decay')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='learning rate')\n",
    "parser.add_argument(\"--hid_feats\", type=int, default=1500, help='Hidden layer dimensionalities.')\n",
    "parser.add_argument(\"--out_feats\", type=int, default=901, help='Output layer dimensionalities.')\n",
    "parser.add_argument(\"--method\", default='sum', help='Merge feature method')\n",
    "parser.add_argument(\"--num_layers\", type=int, default=2, help='Number of GNN layers.')\n",
    "parser.add_argument(\"--input_dropout\", type=float, default=0, help='Dropout applied at input layer.')\n",
    "parser.add_argument(\"--layer_dropout\", type=float, default=0, help='Dropout applied at hidden layers.')\n",
    "parser.add_argument('--random_seed', type=int, default=123, help='random seed')\n",
    "parser.add_argument('--k', type=int, default=4, help='k order')\n",
    "parser.add_argument('--early_stopping', type=int, default=200, help='stop')\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='dropout')\n",
    "parser.add_argument('--mlp', type=list, default=[64, 2], help='mlp layers')\n",
    "parser.add_argument('--neighbor', type=int, default=20, help='neighbor')\n",
    "parser.add_argument('--save_score', default='True', help='save_score')\n",
    "\n",
    "args = parser.parse_args()  \n",
    "args.dd2 = True\n",
    "args.data_dir = 'dataset/'\n",
    "args.result_dir = 'result/'  \n",
    "args.save_score = True if str(args.save_score) == 'True' else False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading():\n",
    "    data = dict()\n",
    "   \n",
    "    data['all_sample'] = pd.read_csv(args.data_dir + 'all_sample.csv', header=None).iloc[:, :].values\n",
    " \n",
    "    data['miRNA'] = pd.read_csv(args.data_dir + 'quchong_bianhao/miRNA.csv', header=None).iloc[:, :].values\n",
    "    data['disease'] = pd.read_csv(args.data_dir + 'quchong_bianhao/disease.csv', header=None).iloc[:, :].values\n",
    "    data['miRNA_disease'] = np.concatenate((data['miRNA'], data['disease']), axis=0)\n",
    "   \n",
    "    data['miRNA_disease_feature'] = pd.read_csv(args.data_dir + 'miRNA_disease_feature.csv', header=None).iloc[:,\n",
    "                                    :].values\n",
    "    \n",
    "    miRNA_embedding = np.loadtxt(args.data_dir + 'data/miRNA_embedding.txt',dtype=np.float,delimiter=None,unpack=False)\n",
    "    data['miRNA_embedding'] = miRNA_embedding[:901]\n",
    "\n",
    "   \n",
    "    disease_embedding = np.loadtxt(args.data_dir + 'data/disease_embedding.txt',dtype=np.float,delimiter=None,unpack=False)\n",
    "    data['disease_embedding'] = disease_embedding[:877]\n",
    "    data['miRNA_disease_embedding'] = np.concatenate((data['miRNA_embedding'], data['disease_embedding']), axis=0)\n",
    "    data['inter_miRNA_disease_feature'] = np.concatenate((data['miRNA_disease_feature'], data['miRNA_disease_embedding']), axis=1)\n",
    "    return data\n",
    "\n",
    "def make_index(data, sample):\n",
    "    sample_index = []\n",
    "    for i in range(sample.shape[0]):\n",
    "        idx = np.where(sample[i][0] == data['miRNA_disease'])\n",
    "        idy = np.where(sample[i][1] == data['miRNA_disease'])\n",
    "        sample_index.append([idx[0].item(), idy[0].item()])\n",
    "    sample_index = np.array(sample_index)\n",
    "    return sample_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rfq\\bio\\MDA\\MHGTMDA\\HGTMDA\\src\\x.ipynb 单元格 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rfq/bio/MDA/MHGTMDA/HGTMDA/src/x.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m loading()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rfq/bio/MDA/MHGTMDA/HGTMDA/src/x.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# args.m_drug_d_num = dataset['m_drug_d_sample'].shape[0]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rfq/bio/MDA/MHGTMDA/HGTMDA/src/x.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# args.m_mRNA_d_num = dataset['m_mRNA_d_sample'].shape[0]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rfq/bio/MDA/MHGTMDA/HGTMDA/src/x.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# args.m_incRNA_d_num = dataset['m_incRNA_d_sample'].shape[0]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rfq/bio/MDA/MHGTMDA/HGTMDA/src/x.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dataset[\u001b[39m'\u001b[39m\u001b[39minter_miRNA_disease_feature\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mFloatTensor(dataset[\u001b[39m'\u001b[39m\u001b[39minter_miRNA_disease_feature\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32mc:\\Users\\rfq\\bio\\MDA\\MHGTMDA\\HGTMDA\\src\\x.ipynb 单元格 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rfq/bio/MDA/MHGTMDA/HGTMDA/src/x.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloading\u001b[39m():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rfq/bio/MDA/MHGTMDA/HGTMDA/src/x.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rfq/bio/MDA/MHGTMDA/HGTMDA/src/x.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mall_sample\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(args\u001b[39m.\u001b[39mdata_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mall_sample.csv\u001b[39m\u001b[39m'\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m.\u001b[39miloc[:, :]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rfq/bio/MDA/MHGTMDA/HGTMDA/src/x.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mmiRNA\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(args\u001b[39m.\u001b[39mdata_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mquchong_bianhao/miRNA.csv\u001b[39m\u001b[39m'\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m.\u001b[39miloc[:, :]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rfq/bio/MDA/MHGTMDA/HGTMDA/src/x.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mdisease\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(args\u001b[39m.\u001b[39mdata_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mquchong_bianhao/disease.csv\u001b[39m\u001b[39m'\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m.\u001b[39miloc[:, :]\u001b[39m.\u001b[39mvalues\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = loading()\n",
    "# args.m_drug_d_num = dataset['m_drug_d_sample'].shape[0]\n",
    "# args.m_mRNA_d_num = dataset['m_mRNA_d_sample'].shape[0]\n",
    "# args.m_incRNA_d_num = dataset['m_incRNA_d_sample'].shape[0]\n",
    "dataset['inter_miRNA_disease_feature'] = t.FloatTensor(dataset['inter_miRNA_disease_feature']).to(device)\n",
    "\n",
    "model = MDA(args).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=args.wd, lr=args.lr)\n",
    "scheduler = t.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)\n",
    "cross_entropy = nn.BCELoss(reduction='mean')\n",
    "file_num = 1\n",
    "\n",
    "\n",
    "auc = 0\n",
    "auprc = 0\n",
    "acc = 0\n",
    "f1 = 0\n",
    "recall = 0\n",
    "pre = 0\n",
    "\n",
    "\n",
    "max_test_acc = 0\n",
    "\n",
    "k = 1\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "for train_index, test_index in kfold.split(dataset['all_sample'][:, :2]):\n",
    "    tran_sample = dataset['all_sample'][train_index][:, :2]\n",
    "    tran_sample_index = make_index(dataset, tran_sample)\n",
    "    tran_label = dataset['all_sample'][train_index][:, 2]\n",
    "    test_sample = dataset['all_sample'][test_index][:, :2]\n",
    "    test_sample_index = make_index(dataset, test_sample)\n",
    "    test_label = dataset['all_sample'][test_index][:, 2]   \n",
    "    tran_sample_index = t.FloatTensor(tran_sample_index).to(device)\n",
    "    tran_label = t.FloatTensor(tran_label.astype(int)).to(device)\n",
    "    test_sample_index = t.FloatTensor(test_sample_index).to(device)\n",
    "    test_label = t.FloatTensor(test_label.astype(int)).to(device)\n",
    "    for i in range(args.epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_score, test_score = model(dataset, tran_sample_index, test_sample_index, device)\n",
    "        train_score = train_score.squeeze(1)\n",
    "\n",
    "        train_label = tran_label.to(device)\n",
    "\n",
    "        train_loss = cross_entropy(train_score, train_label)\n",
    "        train_loss.backward()\n",
    "        train_auc = roc_auc_score(train_label.detach().cpu().numpy(),\n",
    "                                    train_score.detach().cpu().numpy())\n",
    "        train_acc = accuracy_score(train_label.detach().cpu().numpy().astype(np.int64),\n",
    "                                    np.rint(train_score.detach().cpu().numpy()).astype(np.int64))\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "        test_score = test_score.squeeze(1)\n",
    "        test_label = test_label.to(device)\n",
    "        # test_loss = cross_entropy(test_score, test_label)\n",
    "        test_auc = roc_auc_score(test_label.detach().cpu().numpy(),\n",
    "                                    test_score.detach().cpu().numpy())\n",
    "        test_acc = accuracy_score(test_label.detach().cpu().numpy().astype(np.int64),\n",
    "                                    np.rint(test_score.detach().cpu().numpy()).astype(np.int64))\n",
    "        test_aupr = average_precision_score(test_label.detach().cpu().numpy(), test_score.detach().cpu().numpy())\n",
    "        test_f1 = f1_score(test_label.detach().cpu().numpy(),\n",
    "                            np.rint(test_score.detach().cpu().numpy()).astype(np.int64), average='macro')\n",
    "        test_recall = recall_score(test_label.detach().cpu().numpy(),\n",
    "                                    np.rint(test_score.detach().cpu().numpy()).astype(np.int64), average='macro')\n",
    "        test_pre = precision_score(test_label.detach().cpu().numpy(),\n",
    "                                    np.rint(test_score.detach().cpu().numpy()).astype(np.int64), average='macro')\n",
    "\n",
    "        \n",
    "        if test_acc > max_test_acc:\n",
    "            t.save(model.state_dict(), \"./save_model/5_fold/train_model.pth\")\n",
    "            max_test_acc = test_acc\n",
    "            auc = test_auc\n",
    "            auprc = test_aupr\n",
    "            acc = test_acc\n",
    "            f1 = test_f1\n",
    "            recall = test_recall\n",
    "            pre = test_pre\n",
    "            print(f'Epoch: {i + 1:03d}/{args.epochs:03d}' f'   | Learning Rate {scheduler.get_last_lr()[0]:.6f}')\n",
    "            # print(f'Epoch: {i + 1:03d}/{args.epochs:03d}')\n",
    "            print(f'Train Auc.: {train_auc:.4f}' f' | Test Auc.: {test_auc:.4f}')\n",
    "            print(f'Train Loss.: {train_loss.item():.4f}')\n",
    "            print(f'Train Acc.: {train_acc:.4f}' f' | Test Acc.: {test_acc:.4f}')\n",
    "        \n",
    "        scheduler.step()\n",
    "    # file_num += 1\n",
    "    # k += 1\n",
    "    # if k > 1:\n",
    "    #     break\n",
    "print(f' | Test Auc.: {auc:.4f}')\n",
    "print(f' | Test Auprc.: {auprc:.4f}')\n",
    "print(f' | Test Acc.: {acc:.4f}')\n",
    "print(f' | Test F1.: {f1:.4f}')\n",
    "print(f' | Test Recall.: {recall:.4f}')\n",
    "print(f' | Test Precision.: {pre:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
